# ğŸŸ¢ Moroccan-Political-Insight

**Moroccan-Political-Insight** is a comprehensive project designed to analyze and visualize political opinions and trends in Morocco. It implements a **real-time Big Data pipeline**, leverages a **large language model (Mistral 7B)** for tweet classification. Finally, the results are presented in an interactive **Streamlit dashboard**.  

---

## ğŸ“Œ Project Overview

This project provides insights into Moroccan political discussions on social media by analyzing public tweets related to political parties, leaders, and government actions. The pipeline includes:

1. **Data ingestion**: Tweets are collected and stored in Firebase.  
2. **Real-time streaming & cleaning**: Data is streamed from Firebase to Kafka, processed with Spark Streaming, and stored in Hadoop HDFS.  
3. **Export cleaned data**: Cleaned JSON tweets are exported from Hadoop to local storage using WinSCP.  
4. **AI-based classification**: Tweets are classified using the **Mistral 7B** model to detect support or criticism toward political parties.  
5. **Interactive dashboard**: A Streamlit app visualizes trends, keyword frequencies, and classification results.  
 


---

## ğŸ—‚ï¸ Project Structure
```
Moroccan-Political-Insight/
â”‚â”€â”€ README.md
â”‚â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ producer/                     # Data ingestion
â”‚   â””â”€â”€ firebase_producer.py
â”‚
â”œâ”€â”€ streaming/                    # Real-time cleaning
â”‚   â””â”€â”€ stream_to_hdfs.py
â”‚
â”œâ”€â”€ data/                         # Raw input data
â”‚   â””â”€â”€ tweets.json
â”‚
â”œâ”€â”€ exports/                      # Data exported from Hadoop
â”‚   â””â”€â”€ cleaned_tweets.json
â”‚
â”œâ”€â”€ LLM_classification/               # AI classification (Colab)
â”‚   â””â”€â”€ mistral_classification.ipynb
â”‚
â”œâ”€â”€ classified/                   # Tweets classified by LLM (Mistral 7B)
â”‚   â””â”€â”€ classified_tweets.csv
â”‚
â”œâ”€â”€ dashboard/                    # Visualization
â”‚   â””â”€â”€ app.py
â”‚
â””â”€â”€ commands/                     # Hadoop, Kafka & Spark commands
    â””â”€â”€ hadoop_commands.txt

```

---

## ğŸ› ï¸ Key Components

### 1ï¸âƒ£ Producer: `firebase_producer.py`
- Reads raw tweets from Firebase Realtime Database.  
- Publishes tweets to a Kafka topic (`json-topic`) for real-time streaming.

### 2ï¸âƒ£ Streaming: `stream_to_hdfs.py`
- Spark Structured Streaming job that:
  - Reads tweets from Kafka.  
  - Cleans the text (removes extra spaces, special characters).  
  - Saves the cleaned data to **HDFS** in JSON format.  

### 3ï¸âƒ£ Exports
- `cleaned_tweets.json` first exports the cleaned JSON from HDFS (Hortonworks Sandbox VM) to local VM using :
 ``` hdfs dfs -get /user/root/cleaned_tweets_output_json/part-00000-*.json /root/cleaned_tweets.json```
- Then, the file is copied from the Hadoop VM to Windows using WinSCP.
- This ensures the cleaned dataset is available locally for LLM classification.
 
### 4ï¸âƒ£ Classification: `mistral_classification.ipynb`
- Loads **cleaned tweets**.  
- Uses **Mistral 7B LLM** for classification into political labels (pro/contre parties).  
- Generates a CSV with `tweet` and `classe` columns.

### 5ï¸âƒ£ Dashboard: `app.py`
- Streamlit-based interactive dashboard.  
- Visualizes:
  - Tweet distribution by political class.  
  - Top keywords in positive/negative tweets.
  - - Interactive trend graphs. 

### 6ï¸âƒ£ Commands: `hadoop_commands.txt`
- Provides **step-by-step commands** for:
  - Starting Zookeeper and Kafka.  
  - Creating Kafka topics.  
  - Running Spark Streaming.  
  - Exporting cleaned tweets from HDFS.

---

## âš¡ Workflow / Pipeline
```
Firebase (raw tweets)
      â”‚
      â–¼
Kafka Topic ("json-topic")
      â”‚
      â–¼
Spark Streaming â†’ HDFS (cleaned_tweets.json)
      â”‚
      â–¼
Export from HDFS â†’ Local Hadoop VM â†’ Windows (WinSCP)
      â”‚
      â–¼
LLM Classification (Mistral 7B, in Colab)  â†’ classified/classified_tweets.csv
      â”‚
      â–¼
Streamlit Dashboard (app.py)
```
---
## ğŸ› ï¸ Tech Stack

- **Data sources & ingestion**: Firebase Realtime Database

- **Streaming & Processing**: Kafka, Apache Spark Streaming, Hadoop HDFS

- **LLM Classification**: Mistral 7B (executed in Google Colab)

- **Visualization**: Streamlit

- **Utilities** : WinSCP (for file transfer from Hadoop VM to Windows)

 ---

### ğŸ“ Note on the Hadoop Environment

- The entire Big Data pipeline was executed in **Hortonworks Sandbox (Hadoop VM)**.  
- Tweets collected from **Firebase** were **streamed in real-time to Kafka** on the Sandbox.  
- **Spark Streaming** processed these tweets in the Sandbox and stored them in **HDFS**.  
- The cleaned JSON file (`cleaned_tweets.json`) was then **exported from HDFS to the local file system of the Sandbox**, and subsequently transferred to **Windows** via **WinSCP** for LLM classification in Colab.  

---
âš ï¸ Note: LLM classification is executed in Google Colab, which provides GPU acceleration for the Mistral 7B model.
---
  
##  ğŸ‘©â€ğŸ’» Author

**Meriam Sikini**
