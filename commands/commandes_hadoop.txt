üóÇÔ∏è Commandes Hadoop, Spark & Kafka utilis√©es dans le projet
üöÄ Ordre d‚Äôex√©cution des commandes

1- D√©marrer Zookeeper

cd /usr/hdp/current/kafka-broker/config
./zookeeper-server-start.sh ../config/zookeeper.properties


==> Lance le service Zookeeper, n√©cessaire pour g√©rer Kafka.

2- D√©marrer Kafka

cd /usr/hdp/current/kafka-broker/config
./kafka-server-start.sh ../config/server.properties


==> D√©marre le broker Kafka qui √©coutera sur le port 6667.

3- Cr√©er un topic Kafka nomm√© json-topic

./kafka-topics.sh --create --zookeeper sandbox-hdp.hortonworks.com:2181 \
  --replication-factor 1 --partitions 1 --topic json-topic


==> Cr√©e le topic qui recevra les donn√©es depuis Firebase.

4- √âcrire le script PySpark spark_streaming_to_hdfs.py
   vi spark_streaming_firebase_kafka.py 


==> Ce script consomme depuis Kafka, nettoie les tweets et stocke dans HDFS.
(Sauvegarder le fichier sur /root/spark_streaming_to_hdfs.py).

5- Lancer le script Spark Streaming

spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 \
  /root/spark_streaming_to_hdfs.py


==> Spark lit le topic Kafka en continu et √©crit les donn√©es nettoy√©es dans HDFS.

6- V√©rifier les donn√©es dans HDFS 
hdfs dfs -ls /user/root/cleaned_tweets_output_json
hdfs dfs -text /user/root/cleaned_tweets_output_json/part-*.json | head -n 20


==> Permet de contr√¥ler que Spark a bien produit des fichiers JSON dans HDFS.

7- Exporter un fichier JSON nettoy√© de HDFS vers local

hdfs dfs -get /user/root/cleaned_tweets_output_json/part-00000-*.json /root/cleaned_tweets.json


==> Copie un fichier nettoy√© depuis HDFS vers le syst√®me de fichiers local.